# ğŸ¯ README - Start Here!# ğŸ” AgentMonitor - Complete Research Implementation# ğŸ” AgentMonitor - Predictive Multi-Agent System Framework# AgentMonitor: Multi-Agent System Performance Prediction# ğŸ¤– AgentMonitor - Multi-Agent System Monitoring Framework



**Last Updated**: October 11, 2025  

**Status**: âœ… Production Ready - Give to Your Friend!

> **Production-ready** Multi-Agent System framework with enhancement loops, 16-feature extraction, and XGBoost prediction  

---

> Based on research paper: *AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems* (arXiv:2408.14972)

## ğŸ“š Documentation Guide

> A production-ready implementation based on the research paper: *AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems* (arXiv:2408.14972)

### ğŸ‘‹ **For Your Friend** (Data Generation)

---

1. **START HERE**: `FRIEND_QUICK_START.md`

   - Complete step-by-step instructions

   - Setup, run, troubleshooting

   - Read this first!## ğŸš€ Quick Start (3 Steps)



2. **CHECKLIST**: `FRIEND_CHECKLIST.md`**TL;DR:** Monitor your Multi-Agent Systems, extract 16 performance features, train XGBoost to predict MAS quality, and optimize before expensive evaluation.## ğŸ¯ Overview**A Python framework for monitoring, scoring, and optimizing Multi-Agent Systems (MAS) using LLM-based evaluation and XGBoost predictions.**

   - Print this out!

   - Track 50+ runs### 1. Install

   - Simple progress tracker

```bash

### ğŸ“ **For You** (Model Training & Usage)

pip install -r requirements.txt

1. **MAIN GUIDE**: `COMPLETE_GUIDE.md`

   - Complete technical documentation```---

   - All features, workflows, code explanations

   - Reference for everything



---### 2. Set API Key



## âš¡ Quick Commands```bash



### Your Friend Does (50+ times):echo GEMINI_API_KEY=your_key_here > .env## ğŸ“‹ Table of Contents**AgentMonitor** is a machine learning system that predicts Multi-Agent System (MAS) performance using XGBoost regression. It monitors MAS execution, extracts behavioral features, and predicts overall system effectiveness based on agent interactions and collective behavior.**Status**: âœ… **FULLY FUNCTIONAL**  

```bash

python main.py generate```

```



### You Do (ONCE after getting data):

```bash### 3. Run Complete Demo

python main.py train

``````bash- [Quick Start](#-quick-start)**Date**: October 8, 2025  



### You Do (FOREVER after training):python run_complete_agentmonitor.py

```bash

python main.py predict```- [What This Does](#-what-this-does)

```



---

**This runs the COMPLETE system:**- [Installation](#-installation)### Key Innovation**Author**: Kumaraswamy Bakkashetti

## ğŸ“ Project Structure

- âœ… 4-agent MAS pipeline (Analyzer â†’ Coder â†’ Tester â†’ Reviewer)

```

Final/- âœ… **Enhancement loops** (auto-retry if quality < threshold)- [Basic Usage](#-basic-usage)

â”œâ”€â”€ ğŸ“˜ COMPLETE_GUIDE.md          # Your complete documentation

â”œâ”€â”€ ğŸ‘‹ FRIEND_QUICK_START.md      # Friend's instructions- âœ… Extract **16 features** (system + graph + collective)

â”œâ”€â”€ âœ… FRIEND_CHECKLIST.md         # Friend's tracking sheet

â”œâ”€â”€ ğŸ“„ README.md                   # This file- âœ… Show XGBoost training/prediction- [Project Structure](#-project-structure)Instead of evaluating MAS on expensive benchmark tasks, AgentMonitor predicts performance by analyzing:

â”‚

â”œâ”€â”€ ğŸ main.py                     # Main script (3 modes)

â”œâ”€â”€ âš™ï¸ requirements.txt            # Dependencies

â”œâ”€â”€ ğŸ”‘ .env                        # API key (CHANGE THIS!)---- [Complete Workflow](#-complete-workflow)

â”‚

â”œâ”€â”€ ğŸ“ AgentMonitor/               # Framework (19 Python files)

â”‚   â”œâ”€â”€ core/                     # Monitoring

â”‚   â”œâ”€â”€ features/                 # Feature extraction## ğŸ’¡ What Makes This Complete?- [API Reference](#-api-reference)- **System metrics**: Agent scores, enhancement loops, latency, token usage---

â”‚   â”œâ”€â”€ evaluation/               # Benchmarks

â”‚   â”œâ”€â”€ models/                   # Predictor code

â”‚   â”œâ”€â”€ mas/                      # MAS implementations

â”‚   â””â”€â”€ utils/### âœ… Research Paper Features- [Important Notes](#-important-notes)

â”‚

â”œâ”€â”€ ğŸ“ models/                     # Trained models (empty until you train)- **16 Performance Indicators** (6 system + 9 graph + 1 collective)

â”‚   â””â”€â”€ (mas_predictor.pkl after training)

â”‚- **Weak Supervision** (0.5Ã—HumanEval + 0.3Ã—GSM8K + 0.2Ã—MMLU)- [Citation](#-citation)- **Graph features**: Interaction network topology (clustering, centrality, entropy)

â”œâ”€â”€ ğŸ“ data/                       # Training data

â”‚   â””â”€â”€ training_data.csv        # Generated by friend- **XGBoost Regression** with hyperparameter tuning

â”‚

â””â”€â”€ ğŸ“ BenchmarkDatasetFolder/     # Benchmark datasets- **Spearman Correlation** as primary metric

    â”œâ”€â”€ HumanEval/

    â”œâ”€â”€ GSM8K/- **Non-invasive Monitoring** (no code changes needed)

    â””â”€â”€ MMLU/

```---- **Collective behavior**: Overall system coordination score## ğŸ“‹ Table of Contents



---### âœ… Production Features (NEW!)



## ğŸš€ The Complete Workflow- **Enhancement Loops** - Auto-retry if agent output quality < threshold



### Phase 1: Your Friend Generates Data- **LLM-based Scoring** - Quality assessment for each agent output

```

Friend receives:- **Feedback Generation** - Actionable suggestions for improvement## ğŸš€ Quick Start

  â†“

Sets up environment (5 min)- **Conversation Graph** - Track agent interactions automatically

  â†“

Runs: python main.py generate (50+ times, ~1 hour each)- **Comprehensive Logging** - JSON output with full statistics

  â†“

Sends you: data/training_data.csv

```

---### 1. Install Dependencies### Architecture1. [Overview](#overview)

### Phase 2: You Train Model (ONCE)

```

You receive: data/training_data.csv

  â†“## ğŸ“– Complete Usage Example

Run: python main.py train (5 minutes)

  â†“

Creates: models/mas_predictor.pkl âœ…

``````python```bash2. [System Architecture](#system-architecture)



### Phase 3: You Predict (FOREVER)from AgentMonitor.core.enhanced_monitor import EnhancedAgentMonitor

```

Run: python main.py predict (8 seconds)pip install -r requirements.txt

  â†“

Loads: models/mas_predictor.pkl âœ…# Create monitor with enhancement loops

  â†“

Output: Predicted MAS Scoremonitor = EnhancedAgentMonitor(``````3. [Quick Start](#quick-start)

```

    api_key="your_key",

**Key**: Train ONCE, predict FOREVER! No retraining needed!

    threshold=0.6,    # Retry if score < 0.6

---

    max_retries=2,    # Max 2 enhancement attempts

## ğŸ What to Give Your Friend

    debug=True### 2. Set API Keyâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”4. [LLM Integration (Gemini & Ollama)](#llm-integration)

### Required Files:

```)

âœ… Final/ folder (entire thing)

âœ… Tell them to read: FRIEND_QUICK_START.md

âœ… Tell them to print: FRIEND_CHECKLIST.md

âœ… Tell them to change API key in .env# Run agent with automatic enhancement

```

result = await monitor.run_agent_with_enhancement(Create `.env` file:â”‚                    AgentMonitor Pipeline                     â”‚5. [Components](#components)

### Do NOT share:

```    agent=my_coder_agent,

âŒ venv/ folder (they create their own)

âŒ Your API key    task="Write a function to calculate Fibonacci sequence",```bash

```

    agent_name="Coder",

---

    capability="gemini"GEMINI_API_KEY=your_gemini_api_key_hereâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤6. [Usage Guide](#usage-guide)

## ğŸ”§ Installation (Quick)

)

```bash

# 1. Create virtual environment```

python -m venv venv

print(f"Output: {result['output']}")

# 2. Activate it

venv\Scripts\activate          # Windowsprint(f"Score: {result['score']:.2f}")â”‚                                                               â”‚7. [Fixes Applied](#fixes-applied)

source venv/bin/activate       # Mac/Linux

print(f"Attempts: {result['attempts']}")

# 3. Install dependencies

pip install -r requirements.txtprint(f"Enhanced: {result['enhanced']}")### 3. Run Demo



# 4. Set API key in .env

echo GEMINI_API_KEY=your_key > .env

# Save monitoring dataâ”‚  1. MAS Execution                                            â”‚8. [Data Analysis](#data-analysis)

# 5. Ready!

```monitor.save("output.json")



---```bash



## ğŸ“Š Two "models" Folders Explained# Print summary statistics



People get confused by this - here's why both exist:monitor.print_summary()python examples/complete_demo.pyâ”‚     â”œâ”€ Run multi-agent pipeline (code/QA tasks)             â”‚9. [Troubleshooting](#troubleshooting)



1. **`Final/models/`** (folder, empty initially)```

   - Stores **saved trained models** (`.pkl` files)

   - Created after running `python main.py train````

   - This is where `mas_predictor.pkl` lives

**Output:**

2. **`AgentMonitor/models/`** (Python package)

   - Contains **source code** (`predictor.py`)```â”‚     â”œâ”€ Monitor agent interactions                            â”‚

   - The actual training/prediction logic

   - Part of the framework[Coder] âš ï¸ Score 0.52 < 0.60 - Retry 1/2



**Think of it**:[Coder] âœ… Score 0.78 >= 0.60 (attempt 1)---

- `models/` = Library (where books are stored)

- `AgentMonitor/models/` = Printing press (where books are made)



**Both are needed!** Not duplicates!Output: def fibonacci(n):...â”‚     â””â”€ Log execution traces                                  â”‚---



---Score: 0.78



## âœ… VerificationAttempts: 1## ğŸ’¡ What This Does



### Is Everything Working?Enhanced: True



```bash```â”‚                                                               â”‚

# Test imports

python -c "from AgentMonitor import EnhancedAgentMonitor; print('âœ…')"



# Check structure---### Problem

python -c "import os; print('âœ…' if os.path.exists('AgentMonitor/models/predictor.py') else 'âŒ')"



# Verify dependencies

pip list | findstr "pandas xgboost"  # Windows## ğŸ—ï¸ Project StructureEvaluating Multi-Agent Systems (MAS) on benchmarks like HumanEval, GSM8K, MMLU is **expensive** and **time-consuming**. You need to predict which MAS configurations will perform well **before** running full evaluations.â”‚  2. Feature Extraction                                       â”‚## ğŸ¯ Overview

pip list | grep "pandas\|xgboost"   # Mac/Linux

```



All show âœ…? You're good!```



---AgentMonitor/Final/



## ğŸ› Common Issuesâ”œâ”€â”€ run_complete_agentmonitor.py   â­ Main demo (start here!)### Solutionâ”‚     â”œâ”€ System Features (6): scores, loops, latency, tokens  â”‚



### "Module not found"â”‚

```bash

# Activate venv first!â”œâ”€â”€ AgentMonitor/                   ğŸ“¦ Framework package**AgentMonitor** provides:

venv\Scripts\activate

pip install -r requirements.txtâ”‚   â”œâ”€â”€ core/

```

â”‚   â”‚   â”œâ”€â”€ agent_monitor.py        - Basic monitoringâ”‚     â”œâ”€ Graph Features (9): topology, centrality, entropy    â”‚AgentMonitor is an intelligent monitoring system for Multi-Agent Systems that:

### "No API key"

```bashâ”‚   â”‚   â”œâ”€â”€ enhanced_monitor.py     - With enhancement loops â­

# Edit .env file

# Add: GEMINI_API_KEY=your_actual_keyâ”‚   â”‚   â””â”€â”€ agent_wrapper.py        - Simple agent wrapper1. **Non-invasive Monitoring** - Wrap your MAS without changing code

```

â”‚   â”œâ”€â”€ features/

### "Model not trained"

```bashâ”‚   â”‚   â””â”€â”€ feature_extractor.py    - 16 feature extraction2. **16 Performance Features** - Extract system, graph, and collaboration metricsâ”‚     â””â”€ Collective Score (1): coordination measure           â”‚

# You need to train first!

python main.py trainâ”‚   â”œâ”€â”€ evaluation/

```

â”‚   â”‚   â”œâ”€â”€ benchmark_evaluator.py  - HumanEval/GSM8K/MMLU3. **Benchmark Evaluation** - Robust HumanEval/GSM8K/MMLU scoring

### "No training data"

```bashâ”‚   â”‚   â””â”€â”€ mas_orchestrator.py     - Full evaluation pipeline

# Your friend needs to generate data first!

# Or run: python main.py generateâ”‚   â”œâ”€â”€ models/4. **XGBoost Prediction** - Train model to predict MAS performanceâ”‚                                                               â”‚- **Monitors**: Tracks agent outputs with 6-dimensional scoring (factual accuracy, clarity, safety, code correctness, complexity, personal score)

```

â”‚   â”‚   â””â”€â”€ predictor.py            - XGBoost training/prediction

---

â”‚   â””â”€â”€ utils/5. **Variant Optimization** - Test many configurations, deploy only the best

## ğŸ“ˆ Expected Results

â”‚

### After Friend's Work (50+ runs):

- âœ… `data/training_data.csv` existsâ”œâ”€â”€ MAS/â”‚  3. Benchmark Evaluation (Training Data Generation)          â”‚- **Enhances**: Automatically improves low-scoring outputs through iterative refinement

- âœ… Has 50+ rows (samples)

- âœ… Has 20 columns (16 features + 4 scores)â”‚   â””â”€â”€ mas_pipeline.py             - Example MAS implementations



### After Your Training (once):â”‚### Research Paper Results

- âœ… `models/mas_predictor.pkl` exists

- âœ… Terminal shows: "Model saved to models/mas_predictor.pkl"â”œâ”€â”€ scripts/

- âœ… Spearman correlation > 0.8 (with enough data)

â”‚   â””â”€â”€ generate_mas_variants.py    - Generate 30-50 MAS configs- **Spearman Correlation:** 0.89 (in-domain), 0.58 (cross-task)â”‚     â”œâ”€ HumanEval: Code generation tasks                     â”‚- **Collects Features**: Extracts 19 system + graph metrics (latency, tokens, loops, graph centrality, PageRank entropy)

### After Your Prediction (forever):

- âœ… Terminal shows: "Model loaded from models/mas_predictor.pkl"â”‚

- âœ… Takes ~8 seconds (vs hours of benchmark evaluation!)

- âœ… Outputs predicted MAS scoreâ”œâ”€â”€ BenchmarkDatasetFolder/         - Benchmark datasets- **Training Data:** 1,796 MAS variants



---â”‚   â”œâ”€â”€ HumanEval/data.csv



## ğŸ“ Paper Informationâ”‚   â”œâ”€â”€ GSM8k/data.csv- **Benchmarks:** HumanEval (164 samples), GSM8K (100), MMLU (100)â”‚     â”œâ”€ GSM8K: Math reasoning tasks                          â”‚- **Trains ML Models**: Uses XGBoost to predict collective system performance



**Research Paper**: AgentMonitor - Non-invasive MAS Performance Prediction  â”‚   â””â”€â”€ MMLU/data.csv

**arXiv**: 2408.14972  

**Key Innovation**: Predict MAS performance from behavioral features (fast!) instead of benchmark evaluation (slow!)â”‚



**16 Features Extracted**:â”œâ”€â”€ data/                           - Generated data

- 6 System metrics (latency, tokens, scores)

- 9 Graph metrics (centrality, clustering)â””â”€â”€ models/                         - Trained models---â”‚     â”œâ”€ MMLU: Knowledge/QA tasks                             â”‚- **Benchmarks**: Evaluates against HumanEval (code), GSM8K (math), MMLU (knowledge)

- 1 Collective metric (final output quality)

```

**Prediction Model**: XGBoost Regressor  

**Target**: Combined benchmark score (0.5Ã—HumanEval + 0.3Ã—GSM8K + 0.2Ã—MMLU)



------



## ğŸ“ Support## ğŸ“¦ Installationâ”‚     â””â”€ Weak Supervision: label = 0.5Ã—HE + 0.3Ã—GSM + 0.2Ã—MM  â”‚



**For Your Friend**:## ğŸ”„ Complete Workflow

- Read: `FRIEND_QUICK_START.md`

- Print: `FRIEND_CHECKLIST.md`

- Contact you if stuck

### Phase 1: Monitor with Enhancement Loops

**For You**:

- Read: `COMPLETE_GUIDE.md`### Requirementsâ”‚                                                               â”‚### Key Features

- Check: GitHub issues

- File bug reports```python



---# Your agents automatically improve via enhancement loops- Python 3.9+



## ğŸ† Summarymonitor = EnhancedAgentMonitor(api_key="key", threshold=0.6, max_retries=2)



### Your Friend's Job:- Google Gemini API key (or OpenAI/Anthropic)â”‚  4. XGBoost Training                                         â”‚âœ… **Dual LLM Support**: Switch between Google Gemini (cloud) and Ollama (local)  

```bash

python main.py generate  # 50+ times# Run MAS pipeline

# Send: data/training_data.csv

```results = await mas.run(task, monitor)



### Your Job (After Getting CSV):

```bash

python main.py train     # Once# Graph edges tracked automatically### Installâ”‚     â”œâ”€ Input: 16 features (15 + collective_score)           â”‚âœ… **Automated Enhancement**: Iterative improvement loop with configurable threshold  

python main.py predict   # Forever

```monitor.record_graph_edge("Analyzer", "Coder")



### Result:monitor.record_graph_edge("Coder", "Tester")

- âœ… Predict MAS performance in seconds (vs hours)

- âœ… No retraining needed

- âœ… Production-ready ML system

# Save monitoring data```bashâ”‚     â”œâ”€ Target: label_mas_score                              â”‚âœ… **Graph Analytics**: NetworkX-based pipeline topology analysis  

---

monitor.save("output.json")

## ğŸ¯ Next Steps

```# Clone repository

1. **Give folder to friend** â†’ They read `FRIEND_QUICK_START.md`

2. **Friend generates 50+ samples** â†’ Sends you CSV

3. **You train model once** â†’ Saves to `models/mas_predictor.pkl`

4. **You predict forever** â†’ Fast performance estimates!**Key Feature:** If an agent's output scores < 0.6, it automatically:cd AgentMonitor/Finalâ”‚     â””â”€ Output: Trained model (xgb_model.json)               â”‚âœ… **Benchmark Integration**: HumanEval (5,893 tasks), GSM8K (6,142 problems), MMLU  



**That's it!** Simple, clean, production-ready! ğŸš€1. Gets scored by LLM (0-1 scale)



---2. Receives feedback on how to improve



**Questions?** Read `COMPLETE_GUIDE.md` for full details.3. Retries with enhanced prompt



**Ready to start?** Give the `Final/` folder to your friend now!4. Up to max_retries times# Create virtual environmentâ”‚                                                               â”‚âœ… **ML Predictions**: XGBoost regression for performance forecasting  



âœ¨ **Good luck with your research!** âœ¨


---python -m venv venv



### Phase 2: Extract 16 Featuresvenv\Scripts\activate  # Windowsâ”‚  5. Prediction                                               â”‚



```python# source venv/bin/activate  # Linux/Mac

from AgentMonitor.features.feature_extractor import FeatureExtractor

â”‚     â”œâ”€ New MAS â†’ Extract features                           â”‚---

monitor.load("output.json")

extractor = FeatureExtractor(api_key="key")# Install dependencies

features = await extractor.extract_all_features(monitor.monitor_data)

pip install -r requirements.txtâ”‚     â””â”€ Model â†’ Predict performance                          â”‚

# Returns 16 features:

{

    # System Metrics (6)

    'avg_personal_score': 0.82,# Set up API keyâ”‚                                                               â”‚## ğŸ—ï¸ System Architecture

    'min_personal_score': 0.75,

    'max_loops': 2,echo GEMINI_API_KEY=your_key > .env

    'total_latency': 4.5,

    'total_token_usage': 1500,```â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    'num_agents_triggered_enhancement': 1,

    

    # Graph Metrics (9)

    'num_nodes': 4,### Dependencies```### Multi-Agent Pipelines

    'num_edges': 3,

    'avg_clustering': 0.0,- `pandas>=2.0.0` - Data processing

    'global_transitivity': 0.0,

    'avg_degree_centrality': 0.5,- `numpy>=1.24.0` - Numerical computing

    'avg_betweenness_centrality': 0.33,

    'avg_closeness_centrality': 0.67,- `networkx>=3.1` - Graph analysis (9 graph features)

    'pagerank_entropy': 1.38,

    'heterogeneity_score': 0.25,- `xgboost>=2.0.0` - Prediction model---**Code Generation Pipeline** (5 agents):

    

    # Collective Metric (1)- `scikit-learn>=1.3.0` - ML utilities

    'collective_score': 0.80

}- `google-generativeai>=0.3.0` - LLM integration```

```



---

---## ğŸ“ Project StructureRequirementAnalyzer â†’ CodeGenerator â†’ CodeReviewer â†’ UnitTestWriter â†’ CodeExecutor

### Phase 3: Generate Training Data



```bash

# Generate 30-50 MAS variants## ğŸ¯ Basic Usage```

python scripts/generate_mas_variants.py



# Evaluate each variant on benchmarks

# (You implement this based on your benchmarks)### Step 1: Monitor Your MAS```



# Result: data/mas_benchmark_results.csv

# Format: [16 features] + [3 benchmark scores] + [1 label]

``````pythonAgentMonitor/Final/**QA Pipeline** (3 agents):



**Important:** You need **30-50 variants minimum** for meaningful XGBoost training!from AgentMonitor import AgentMonitor, AgentWrapper



| Variants | Spearman | Quality |â”‚```

|----------|----------|---------|

| 2 | 0.0-0.2 | âŒ Useless |# Create monitor

| 30-50 | 0.4-0.6 | âœ… Basic |

| 100+ | 0.6-0.7 | âœ… Good |monitor = AgentMonitor(debug=True)â”œâ”€â”€ Agent_Monitor/                    # Core monitoring systemRequirementAnalyzer â†’ QAAnswerer â†’ QAReviewer

| 1,796 (paper) | 0.89 | ğŸ¯ Excellent |



---

# Create agentsâ”‚   â”œâ”€â”€ agent_monitor.py              # MAS execution monitor```

### Phase 4: Train XGBoost Predictor

agent1 = AgentWrapper(name="Coder", capability="coding", model="gemini-flash")

```python

from AgentMonitor.models.predictor import MASPredictoragent2 = AgentWrapper(name="Reviewer", capability="review", model="gemini-flash")â”‚   â”œâ”€â”€ run_with_monitor.py           # Main entry point for MAS execution



predictor = MASPredictor(model_path="models/mas_predictor.pkl")

metrics = predictor.train(

    data_path="data/mas_benchmark_results.csv",# Register agents (non-invasive wrapper)â”‚   â”œâ”€â”€ feature_aggregator.py         # Feature extraction from MAS runs### Monitoring Workflow

    test_size=0.2,

    tune_hyperparams=Trueawait monitor.register(agent1, agent1.input, agent1.output, capability="coding")

)

await monitor.register(agent2, agent2.input, agent2.output, capability="review")â”‚   â”œâ”€â”€ evaluate_mas_on_benchmarks.py # Benchmark evaluation for training data

print(f"Spearman: {metrics['test_spearman']:.4f}")

print(f"RÂ²: {metrics['test_r2']:.4f}")



# Save trained model# Run your MASâ”‚   â””â”€â”€ utils/```

predictor.save("models/mas_predictor.pkl")

```task = "Write a function to calculate factorial"



---result1 = await agent1.run(task)â”‚       â”œâ”€â”€ eval_utils.py             # Answer evaluation utilitiesUser Prompt



### Phase 5: Predict New MAS Performanceresult2 = await agent2.run(result1)



```pythonâ”‚       â”œâ”€â”€ graph_utils.py            # Graph metric computation    â†“

# Load trained model

predictor.load("models/mas_predictor.pkl")# Save monitoring data



# Extract features from new MASmonitor.save("output.json")â”‚       â””â”€â”€ json_logger.py            # JSON logging utilitiesAgent Pipeline (MAS)

new_features = {...}  # 16 features

```

# Predict (fast!)

predicted_score = predictor.predict(new_features)â”‚    â†“



if predicted_score > 0.7:### Step 2: Extract Features

    print("âœ… Deploy this MAS!")

else:â”œâ”€â”€ MAS/                              # Multi-Agent System pipelinesAgentMonitor (6D Scoring)

    print("âŒ Reject - predicted performance too low")

``````python



---from AgentMonitor import FeatureExtractorâ”‚   â””â”€â”€ mas_pipeline.py               # Code & QA agent pipelines    â†“



## ğŸ¯ Key Features Explained



### 1. Enhancement Loops â­ NEW!# Load monitoring dataâ”‚Enhancement Loop (if score < threshold)



**Problem:** Agents sometimes produce low-quality outputs  monitor.load("output.json")

**Solution:** Automatically retry with feedback

â”œâ”€â”€ Trainer/                          # Model training & prediction    â†“

```python

# Configuration# Extract 16 features

threshold = 0.6   # Quality threshold

max_retries = 2   # Max retry attemptsextractor = FeatureExtractor(api_key="your_api_key")â”‚   â”œâ”€â”€ xgb_trainer_mas.py            # XGBoost training scriptFeature Aggregator (19 metrics)



# Automatic process:features = await extractor.extract_all_features(monitor.monitor_data)

# 1. Agent generates output

# 2. LLM scores quality (0-1)â”‚   â””â”€â”€ predict_mas.py                # Prediction script    â†“

# 3. If score < threshold:

#    - Generate improvement feedbackprint(features)

#    - Retry with enhanced prompt

#    - Repeat up to max_retries# {â”‚XGBoost Model (Prediction)

# 4. Accept best output

```#   'avg_personal_score': 0.85,      # System metrics (6)



**Benefits:**#   'min_personal_score': 0.75,â”œâ”€â”€ BenchmarkDatasetFolder/           # Benchmark datasets    â†“

- âœ… Improves output quality automatically

- âœ… No manual intervention needed#   'max_loops': 2,

- âœ… Tracks enhancement statistics

- âœ… Works with any agent#   'total_latency': 5.3,â”‚   â”œâ”€â”€ HumanEval/CSV Logging + JSON Export



---#   'total_token_usage': 1200,



### 2. 16 Performance Features#   'num_agents_triggered_enhancement': 1,â”‚   â”‚   â””â”€â”€ data.csv                  # Code generation tasks```



Following the research paper exactly:#   'num_nodes': 2,                   # Graph metrics (9)



**System Metrics (6):**#   'num_edges': 1,â”‚   â”œâ”€â”€ GSM8k/

1. `avg_personal_score` - Average LLM-judged agent quality

2. `min_personal_score` - Minimum agent quality (bottleneck)#   'avg_clustering': 0.0,

3. `max_loops` - Maximum enhancement loops used

4. `total_latency` - Total execution time#   'global_transitivity': 0.0,â”‚   â”‚   â””â”€â”€ data.csv                  # Math reasoning tasks### Components Overview

5. `total_token_usage` - Total LLM tokens used

6. `num_agents_triggered_enhancement` - Agents needing retry#   'avg_degree_centrality': 0.5,



**Graph Metrics (9):**#   'avg_betweenness_centrality': 0.0,â”‚   â””â”€â”€ MMLU/

7. `num_nodes` - Number of agents

8. `num_edges` - Number of interactions#   'avg_closeness_centrality': 1.0,

9. `avg_clustering` - Clustering coefficient

10. `global_transitivity` - Graph transitivity#   'pagerank_entropy': 0.69,â”‚       â””â”€â”€ data.csv                  # Knowledge/QA tasks| Component | File | Purpose |

11. `avg_degree_centrality` - Average degree centrality

12. `avg_betweenness_centrality` - Average betweenness#   'heterogeneity_score': 0.0,

13. `avg_closeness_centrality` - Average closeness

14. `pagerank_entropy` - PageRank entropy#   'collective_score': 0.80          # Collective metric (1)â”‚|-----------|------|---------|

15. `heterogeneity_score` - Agent capability diversity

# }

**Collective Metric (1):**

16. `collective_score` - LLM-judged collaboration quality```â”œâ”€â”€ data/                             # Generated data| **MAS Pipeline** | `MAS/mas_pipeline.py` | Multi-agent code/QA task execution |



---



### 3. XGBoost Prediction### Step 3: Evaluate on Benchmarksâ”‚   â”œâ”€â”€ mas_benchmark_results.csv     # Training dataset (5Ã—20)| **Agent Monitor** | `Agent_Monitor/agent_monitor.py` | LLM-based scoring & enhancement |



**Training:**

```python

# Hyperparameter tuning via GridSearchCV```pythonâ”‚   â””â”€â”€ mas_benchmark_results_FORMAT_GUIDE.csv  # Example format| **Feature Aggregator** | `Agent_Monitor/feature_aggregator.py` | 19-metric feature engineering |

param_grid = {

    'max_depth': [3, 5, 7],from AgentMonitor import MASOrchestrator

    'learning_rate': [0.01, 0.1, 0.3],

    'n_estimators': [50, 100, 200],â”‚| **Run Orchestrator** | `Agent_Monitor/run_with_monitor.py` | Main pipeline coordinator |

    'subsample': [0.7, 0.8, 1.0],

    'colsample_bytree': [0.7, 0.8, 1.0],# Create orchestrator

}

orchestrator = MASOrchestrator(api_key="your_api_key")â”œâ”€â”€ models/                           # Trained models| **Benchmark Runner** | `Agent_Monitor/benchmark_runner.py` | Dataset evaluation |

# 5-fold cross-validation

# Optimize for Spearman correlation

```

# Evaluate MAS on benchmarksâ”‚   â””â”€â”€ xgb_model.json                # XGBoost model| **XGBoost Trainer** | `Trainer/xgb_trainer.py` | ML model training |

**Metrics:**

- RMSE (Root Mean Squared Error)results = await orchestrator.evaluate_mas_on_benchmarks(

- MAE (Mean Absolute Error)

- RÂ² Score    mas_variant=your_mas,â”‚| **Predictor** | `Trainer/predict.py` | Performance prediction |

- **Spearman Correlation** â­ (paper's primary metric)

    humaneval_samples=10,

---

    gsm8k_samples=10,â”œâ”€â”€ logs/                             # Execution logs

## ğŸ“Š Example Output

    mmlu_samples=10

### Monitoring Summary

```)â”‚---

==============================================================

AGENT MONITOR SUMMARY

==============================================================

Total Agents: 4print(results)â”œâ”€â”€ requirements.txt                  # Python dependencies

Total Conversations: 12

Total Enhancements: 2# {



Per-Agent Statistics:#   'features': {...},           # 16 featuresâ”œâ”€â”€ README.md                         # This file## ğŸš€ Quick Start

--------------------------------------------------------------

#   'humaneval_score': 0.65,     # Benchmark scores

Analyzer:

  Calls:        1#   'gsm8k_score': 0.72,â””â”€â”€ INSTRUCTIONS_FOR_FRIEND.md        # Dataset generation guide

  Enhancements: 0

  Avg Score:    0.850#   'mmlu_score': 0.58,

  Min Score:    0.850

  Avg Latency:  2.341s#   'label': 0.663               # Weak supervision: 0.5*HE + 0.3*GSM8K + 0.2*MMLU```### Prerequisites

  Tokens:       450

# }

Coder:

  Calls:        1```

  Enhancements: 1

  Avg Score:    0.780

  Min Score:    0.520

  Avg Latency:  3.102s### Step 4: Train Predictor---- Python 3.8+

  Tokens:       680



Tester:

  Calls:        1```python- Virtual environment (venv)

  Enhancements: 0

  Avg Score:    0.720from AgentMonitor import MASPredictor

  Min Score:    0.720

  Avg Latency:  2.856s## ğŸ”§ Features Extracted (16 Total)- Gemini API key OR Ollama installed locally

  Tokens:       520

# Train on evaluation results

Reviewer:

  Calls:        1predictor = MASPredictor(model_path="models/mas_predictor.pkl")

  Enhancements: 1

  Avg Score:    0.810metrics = predictor.train(

  Min Score:    0.580

  Avg Latency:  2.945s    data_path="data/mas_benchmark_results.csv",### System Features (6)### Installation

  Tokens:       590

==============================================================    test_size=0.2,

```

    tune_hyperparams=True| Feature | Description | Range |

### Feature Extraction

```)

==============================================================

EXTRACTED FEATURES (16 Indicators)|---------|-------------|-------|```powershell

==============================================================

print(f"Spearman Correlation: {metrics['test_spearman']:.4f}")

ğŸ“Š System Metrics (6):

   avg_personal_score:              0.7900```| `avg_personal_score` | Average agent performance score | 0-1 |# 1. Clone repository

   min_personal_score:              0.7200

   max_loops:                       1

   total_latency:                   11.2440 sec

   total_token_usage:               2240### Step 5: Predict New MAS| `min_personal_score` | Minimum agent performance score | 0-1 |git clone https://github.com/KumaraswamyBakkashetti/3-1project.git

   num_agents_triggered_enhancement: 2



ğŸ•¸ï¸  Graph Metrics (9):

   num_nodes:                       4```python| `max_loops` | Maximum enhancement loops triggered | 0-10+ |cd Final

   num_edges:                       3

   avg_clustering:                  0.0000# Load trained model

   global_transitivity:             0.0000

   avg_degree_centrality:           0.5000predictor.load("models/mas_predictor.pkl")| `total_latency` | Total execution time (seconds) | 0-âˆ |

   avg_betweenness_centrality:      0.3333

   avg_closeness_centrality:        0.6667

   pagerank_entropy:                1.3863

   heterogeneity_score:             0.0000# Extract features from new MAS variant| `total_token_usage` | Total tokens consumed | 0-âˆ |# 2. Create virtual environment



ğŸ¤ Collective Metric (1):new_features = await extractor.extract_all_features(new_mas_data)

   collective_score:                0.8200

==============================================================| `num_agents_triggered_enhancement` | Count of agents needing enhancement | 0-N |python -m venv venv

```

# Predict performance (fast!)

---

predicted_score = predictor.predict(new_features).\venv\Scripts\Activate.ps1

## âš ï¸ Important Notes



### 1. Minimum Variants Required

if predicted_score > 0.7:### Graph Features (9)

**DO NOT use only 2 MAS variants for training!**

    print("âœ… Deploy this MAS variant!")

The paper used **1,796 variants**. For practical results:

- **Minimum:** 30-50 variantselse:| Feature | Description | Range |# 3. Install dependencies

- **Recommended:** 100+ variants

- **Ideal:** 500-1000 variants    print("âŒ Reject - predicted performance too low")



Use `scripts/generate_mas_variants.py` to create variants automatically.```|---------|-------------|-------|pip install -r requirements.txt



---



### 2. Enhancement Loop Configuration---| `num_nodes` | Number of agent nodes | 3-10 |



**threshold:** Lower = more retries, higher = faster

- 0.5 = Lenient (fewer retries)

- 0.6 = Balanced â­ (recommended)## ğŸ“ Project Structure| `num_edges` | Number of interactions | 2-N |# 4. Configure environment

- 0.7 = Strict (more retries)



**max_retries:** Higher = better quality, slower

- 1 = Fast, basic improvement```| `clustering_coefficient` | Local clustering measure | 0-1 |# Create .env file with:

- 2 = Balanced â­ (recommended)

- 3 = Best quality, slowestAgentMonitor/Final/



---â”œâ”€â”€ AgentMonitor/              # Main framework package| `transitivity` | Global clustering measure | 0-1 |LLM_PROVIDER=gemini  # or "ollama"



### 3. Weak Supervision Formulaâ”‚   â”œâ”€â”€ core/



```pythonâ”‚   â”‚   â”œâ”€â”€ agent_monitor.py   # Non-invasive monitoring| `avg_degree_centrality` | Average node connections | 0-1 |GEMINI_API_KEY=your_api_key_here

label = 0.5 * humaneval_score + 0.3 * gsm8k_score + 0.2 * mmlu_score

```â”‚   â”‚   â””â”€â”€ agent_wrapper.py   # Simple agent abstraction



This creates a single quality metric from multiple benchmarks.â”‚   â”œâ”€â”€ features/| `avg_betweenness_centrality` | Average bridge importance | 0-1 |GEMINI_MODEL=gemini-2.0-flash



---â”‚   â”‚   â””â”€â”€ feature_extractor.py  # 16 feature extraction



## ğŸ¯ Quick Commandsâ”‚   â”œâ”€â”€ evaluation/| `avg_closeness_centrality` | Average node proximity | 0-1 |```



```bashâ”‚   â”‚   â”œâ”€â”€ benchmark_evaluator.py  # HumanEval/GSM8K/MMLU

# Run complete demo (RECOMMENDED)

python run_complete_agentmonitor.pyâ”‚   â”‚   â””â”€â”€ mas_orchestrator.py     # Full evaluation pipeline| `pagerank_entropy` | Information distribution | 0-âˆ |



# Generate MAS variants (for training)â”‚   â”œâ”€â”€ models/

python scripts/generate_mas_variants.py

â”‚   â”‚   â””â”€â”€ predictor.py       # XGBoost trainer & predictor| `heterogeneity_score` | Network diversity | 0-âˆ |### Basic Usage

# Run older examples (if needed)

python examples/complete_demo.pyâ”‚   â””â”€â”€ utils/

python examples/simple_integration.py

â”‚

# Install dependencies

pip install -r requirements.txtâ”œâ”€â”€ examples/

```

â”‚   â”œâ”€â”€ complete_demo.py       # Full end-to-end demo### Collective Score (1)```powershell

---

â”‚   â””â”€â”€ simple_integration.py  # Integration with existing MAS

## ğŸ“– Citation

â”‚| Feature | Description | Range |# Run single task

```bibtex

@article{chan2024agentmonitor,â”œâ”€â”€ scripts/

  title={AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems},

  author={Chan, Chi-Min and Huang, Jianxuan and Liu, Weize and Lyu, Xinle and Liu, Zikang and Yang, Shuyu and Liu, Jiaxuan and Zhou, Yixin and Chen, Qianyu and Wang, Chunyang and others},â”‚   â””â”€â”€ generate_mas_variants.py  # Create 30-50 MAS variants|---------|-------------|-------|python Agent_Monitor/run_with_monitor.py

  journal={arXiv preprint arXiv:2408.14972},

  year={2024}â”‚

}

```â”œâ”€â”€ MAS/| `collective_score` | Overall MAS coordination | 0-1 |# Enter: "write python code for fibonacci sequence"



**Paper:** https://arxiv.org/abs/2408.14972  â”‚   â””â”€â”€ mas_pipeline.py        # Example MAS implementations

**Official Repo:** https://github.com/chanchimin/AgentMonitor

â”‚

---

â”œâ”€â”€ BenchmarkDatasetFolder/

## ğŸ¤ What's New in This Implementation?

â”‚   â”œâ”€â”€ HumanEval/data.csv### Target Labels (4) - Only in Training Data# Run benchmarks (generate training data)

| Feature | Research Paper | This Implementation |

|---------|---------------|-------------------|â”‚   â”œâ”€â”€ GSM8k/data.csv

| 16 Features | âœ… | âœ… |

| XGBoost Prediction | âœ… | âœ… |â”‚   â””â”€â”€ MMLU/data.csv| Label | Description | Formula |python Agent_Monitor/benchmark_runner.py

| Weak Supervision | âœ… | âœ… |

| Non-invasive Monitoring | âœ… | âœ… |â”‚

| **Enhancement Loops** | âŒ | âœ… â­ NEW! |

| **LLM Scoring** | âœ… | âœ… Enhanced |â”œâ”€â”€ data/|-------|-------------|---------|

| **Feedback Generation** | âŒ | âœ… â­ NEW! |

| **Production Logging** | âš ï¸ Basic | âœ… Complete |â”‚   â”œâ”€â”€ mas_variants/          # Generated MAS configs

| **Easy to Use** | âš ï¸ Complex | âœ… Simple API |

â”‚   â””â”€â”€ mas_benchmark_results.csv  # Training data| `humaneval_score` | Code generation accuracy | Mean correctness |# Train XGBoost model

---

â”‚

## ğŸ“„ License

â”œâ”€â”€ models/| `gsm8k_score` | Math reasoning accuracy | Mean correctness |python Trainer/xgb_trainer.py

MIT License

â”‚   â””â”€â”€ mas_predictor.pkl      # Trained XGBoost model

---

â”‚| `mmlu_score` | Knowledge accuracy | Mean correctness |

## ğŸ™ Acknowledgments

â”œâ”€â”€ README.md                  # This file

- Research paper authors for the methodology

- Original AgentMonitor teamâ””â”€â”€ requirements.txt           # Dependencies| `label_mas_score` | **Prediction Target** | 0.5Ã—HE + 0.3Ã—GSM + 0.2Ã—MM |# Make predictions

- NetworkX for graph analysis

- XGBoost team```



---python Trainer/predict.py



**Built with â¤ï¸ for Multi-Agent System research and production**---



*Complete implementation ready for deployment!*---```


## ğŸ”„ Complete Workflow



### Phase 1: Generate MAS Variants (30-50 configs)

## ğŸš€ Installation---

**Why?** Paper used 1,796 variants. Minimum 30-50 for meaningful XGBoost training.



```bash

# Generate 30-50 variants automatically### Prerequisites## ğŸ¦™ LLM Integration (Gemini & Ollama)

python scripts/generate_mas_variants.py

- Python 3.8+

# Output: data/mas_variants/all_variants.json

```- Virtual environment (recommended)### Current Status: Gemini (Cloud API)



Creates variants by varying:- LLM API (Gemini, OpenAI, or Ollama with Llama3)

- Number of agents (2, 3, 4)

- Topology (sequential, parallel, iterative)**Active Configuration**:

- Max loops (1, 2, 3)

- Temperature (0.7, 0.9, 1.1)### Setup- Provider: Google Gemini 2.0 Flash

- Agent roles (different combinations)

- API Key: Configured in `.env`

### Phase 2: Evaluate All Variants

```bash- Model: `gemini-2.0-flash`

```python

from AgentMonitor import MASOrchestrator# Clone repository

import json

git clone <repository-url>### Switching to Ollama (Local)

orchestrator = MASOrchestrator(api_key="your_api_key")

results = []cd AgentMonitor/Final



# Load variants**Why Ollama?**

with open("data/mas_variants/all_variants.json") as f:

    variants = json.load(f)# Create virtual environment- âœ… **Free**: No API costs



# Evaluate each variantpython -m venv venv- âœ… **Private**: Data stays local

for variant in variants:

    mas = create_mas_from_config(variant)  # Your implementation- âœ… **Offline**: Works without internet

    result = await orchestrator.evaluate_mas_on_benchmarks(

        mas_variant=mas,# Activate virtual environment- âœ… **Fast**: Local inference on GPU/CPU

        humaneval_samples=5,  # Small sample for speed

        gsm8k_samples=5,# Windows:

        mmlu_samples=5

    )venv\Scripts\activate**Step-by-Step Integration**:

    results.append(result)

# Linux/Mac:

# Save training data

import pandas as pdsource venv/bin/activate#### 1. Create LLM Wrapper

df = pd.DataFrame(results)

df.to_csv("data/mas_benchmark_results.csv", index=False)Create `Agent_Monitor/llm_wrapper.py`:

```

# Install dependencies```python

### Phase 3: Train XGBoost Model

pip install -r requirements.txtimport os

```python

from AgentMonitor import MASPredictorimport ollama



predictor = MASPredictor()# Set up environment variablesimport google.generativeai as genai

metrics = predictor.train(

    data_path="data/mas_benchmark_results.csv",# Windows PowerShell:

    test_size=0.2,

    tune_hyperparams=True$env:GEMINI_API_KEY = "your-api-key-here"class OllamaClientWrapper:

)

    def __init__(self, model_name="llama3"):

print(f"RMSE: {metrics['test_rmse']:.4f}")

print(f"RÂ²: {metrics['test_r2']:.4f}")# Linux/Mac:        self.model_name = model_name

print(f"Spearman: {metrics['test_spearman']:.4f}")  # Key metric!

export GEMINI_API_KEY="your-api-key-here"    

# Save model

predictor.save("models/mas_predictor.pkl")```    def generate_content(self, prompt: str) -> str:

```

        response = ollama.generate(model=self.model_name, prompt=prompt)

### Phase 4: Use for Prediction

---        return response['response']

```python

# Quick performance check for new MAS

predictor.load("models/mas_predictor.pkl")

## ğŸ’¡ Usageclass GeminiClientWrapper:

new_mas = create_new_mas_variant()

features = extract_features(new_mas)    def __init__(self, api_key: str, model_name="gemini-2.0-flash"):

predicted_score = predictor.predict(features)

### 1. Generate Training Dataset        genai.configure(api_key=api_key)

print(f"Predicted MAS score: {predicted_score:.4f}")

```        self.model_name = model_name



---```bash    



## ğŸ“š API Referencepython Agent_Monitor/evaluate_mas_on_benchmarks.py    def generate_content(self, prompt: str) -> str:



### AgentMonitor```        model = genai.GenerativeModel(self.model_name)



**Core monitoring class**        return model.generate_content(prompt).text



```python**Output**: `data/mas_benchmark_results.csv` (5 rows Ã— 20 columns)

from AgentMonitor import AgentMonitor

def create_llm_client(provider=None, **kwargs):

monitor = AgentMonitor(debug=False)

```- Tests 5 MAS variants on 3 benchmarks    provider = provider or os.getenv("LLM_PROVIDER", "ollama")



**Methods:**- Each MAS runs 30 times (10 samples Ã— 3 benchmarks)    if provider == "ollama":

- `register(agent, input_method, output_method, capability)` - Register agent for monitoring

- `save(filepath)` - Save monitoring data to JSON- Features aggregated across all runs        return OllamaClientWrapper(kwargs.get("model_name", "llama3"))

- `load(filepath)` - Load monitoring data from JSON

- `get_agent_stats()` - Get per-agent statistics- Takes ~30-45 minutes for 10 samples    elif provider == "gemini":

- `get_graph_edges()` - Get conversation graph edges

        return GeminiClientWrapper(kwargs.get("api_key"), kwargs.get("model_name"))

### FeatureExtractor

### 2. Train XGBoost Model```

**Extract 16 performance features**



```python

from AgentMonitor import FeatureExtractor```bash#### 2. Update agent_monitor.py



extractor = FeatureExtractor(api_key="your_key")python Trainer/xgb_trainer_mas.py```python

features = await extractor.extract_all_features(monitor_data)

``````# Replace imports



**Features:**from Agent_Monitor.llm_wrapper import create_llm_client

- **System (6):** avg_personal_score, min_personal_score, max_loops, total_latency, total_token_usage, num_agents_triggered_enhancement

- **Graph (9):** num_nodes, num_edges, avg_clustering, global_transitivity, centrality metrics, pagerank_entropy, heterogeneity**Output**: `models/xgb_model.json`

- **Collective (1):** collective_score (LLM-judged collaboration quality)

# Update __init__

### BenchmarkEvaluator

- Loads training data from `data/mas_benchmark_results.csv`def __init__(self, llm_client=None, threshold=0.6, ...):

**Evaluate MAS on benchmarks**

- Uses 16 features (15 + collective_score) as X    self.client = llm_client

```python

from AgentMonitor import BenchmarkEvaluator- Uses `label_mas_score` as y```



evaluator = BenchmarkEvaluator()- Saves trained model



# HumanEval (code execution)#### 3. Update run_with_monitor.py

score = evaluator.evaluate_humaneval(mas_output, test_cases)

### 3. Make Predictions```python

# GSM8K (numeric answer matching)

score = evaluator.evaluate_gsm8k(mas_output, expected_answer)from Agent_Monitor.llm_wrapper import create_llm_client



# MMLU (text matching)```bash

score = evaluator.evaluate_mmlu(mas_output, correct_answer)

```python Trainer/predict_mas.pydef run_prompt(..., llm_provider=None):



### MASOrchestrator```    llm_client = create_llm_client(provider=llm_provider)



**Full evaluation pipeline**    monitor = AgentMonitor(llm_client=llm_client, ...)



```python**Input**: New MAS features (16 values)  ```

from AgentMonitor import MASOrchestrator

**Output**: Predicted MAS performance score (0-1)

orchestrator = MASOrchestrator(api_key="your_key")

results = await orchestrator.evaluate_mas_on_benchmarks(#### 4. Update .env

    mas_variant=your_mas,

    humaneval_samples=10,---```env

    gsm8k_samples=10,

    mmlu_samples=10LLM_PROVIDER=ollama

)

```## ğŸ“Š Training Data FormatOLLAMA_MODEL=llama3



### MASPredictorOLLAMA_HOST=http://localhost:11434



**XGBoost training and prediction**### CSV Structure (20 columns)```



```python

from AgentMonitor import MASPredictor

```csv#### 5. Install & Setup

predictor = MASPredictor(model_path="models/mas_predictor.pkl")

avg_personal_score,min_personal_score,max_loops,total_latency,total_token_usage,num_agents_triggered_enhancement,num_nodes,num_edges,clustering_coefficient,transitivity,avg_degree_centrality,avg_betweenness_centrality,avg_closeness_centrality,pagerank_entropy,heterogeneity_score,collective_score,humaneval_score,gsm8k_score,mmlu_score,label_mas_score```powershell

# Train

metrics = predictor.train(0.63,0.40,2,25.48,1384,3,5,4,0.18,0.22,0.36,0.14,0.27,2.16,0.008,0.65,0.72,0.58,0.61,0.63# Install Ollama Python package

    data_path="data/mas_benchmark_results.csv",

    test_size=0.2,0.71,0.52,3,32.15,1876,5,6,5,0.22,0.28,0.42,0.18,0.31,2.45,0.011,0.73,0.78,0.65,0.68,0.72pip install ollama

    tune_hyperparams=True

)...



# Predict```# Pull Llama model

score = predictor.predict(features_dict)

ollama pull llama3

# Save/Load

predictor.save("model.pkl")**Breakdown**:

predictor.load("model.pkl")

```- Columns 1-6: System features# Test



---- Columns 7-15: Graph featuresollama run llama3 "Hello"



## âš ï¸ Important Notes- Column 16: Collective score```



### 1. Variant Requirements- Columns 17-19: Benchmark scores (training only)



**DO NOT use only 2 MAS variants!** XGBoost cannot learn patterns from 2 samples.- Column 20: Label (training target)#### 6. Run with Ollama



| Variants | Spearman Correlation | Quality |```powershell

|----------|---------------------|---------|

| 2 | 0.0-0.2 | âŒ Useless |---python Agent_Monitor/run_with_monitor.py

| 10-20 | 0.2-0.3 | âš ï¸ Very weak |

| 30-50 | 0.4-0.6 | âœ… Basic patterns |# Output: [INFO] Using Ollama with model: llama3

| 100+ | 0.6-0.7 | âœ… Good |

| 1,796 (paper) | 0.89 | ğŸ¯ Excellent |## ğŸ§ª MAS Variants Tested```



**Solution:** Use `scripts/generate_mas_variants.py` to create 30-50 variants automatically.



### 2. Weak Supervision FormulaThe system evaluates 5 different MAS configurations:**Performance Comparison**:



The paper uses weighted benchmark scores as labels:



```python| Variant | Threshold | Max Retries | Description || Metric | Gemini | Ollama (Llama3) |

label = 0.5 * humaneval_score + 0.3 * gsm8k_score + 0.2 * mmlu_score

```|---------|-----------|-------------|-------------||--------|--------|----------------|



This creates a single quality metric from multiple benchmarks.| CodeMAS_v1 | 0.6 | 2 | Balanced configuration || Cost | $0.0005/1K tokens | Free |



### 3. LLM-Judged Scores| CodeMAS_Aggressive | 0.5 | 3 | More enhancement loops || Latency | ~2-5s | ~3-10s (CPU), ~1-3s (GPU) |



Personal and collective scores use LLM evaluation:| CodeMAS_Conservative | 0.7 | 1 | Fewer enhancement loops || Privacy | Cloud | Local |

- **Personal scores:** Quality of individual agent outputs (0-1)

- **Collective score:** Overall collaboration quality (0-1)| LogicMAS_v1 | 0.6 | 2 | Logic-focused tasks || Quality | Excellent | Good |



Fallback to heuristics if LLM unavailable.| QA_MAS_v1 | 0.6 | 2 | Question-answering tasks |



### 4. Non-Invasive Monitoring---



AgentMonitor uses **monkey-patching** to wrap agent methods without changing your code:---



```python## ğŸ“¦ Components

# Your existing MAS (NO CHANGES!)

from MAS.mas_pipeline import CodePipeline## ğŸ”¬ How It Works

pipeline = CodePipeline(llm)

### 1. MAS Pipeline (`MAS/mas_pipeline.py`)

# Add monitoring (wrapper)

monitor = AgentMonitor()### Step 1: MAS Execution with Monitoring

# ... wrap agents ...

**Code Pipeline**:

# Run normally

result = pipeline.run(task)```python- `RequirementAnalyzer`: Extracts requirements, identifies language



# Features extracted!from Agent_Monitor.run_with_monitor import run_prompt- `CodeGenerator`: Generates working code with LLM

```

- `CodeReviewer`: Detects bugs and inefficiencies

See `examples/simple_integration.py` for complete example.

data, features, raw_log, summary = run_prompt(- `UnitTestWriter`: Creates test cases

### 5. Expected Performance

    prompt="Write a function to reverse a string",- `CodeExecutor`: Runs syntax checks and simulates execution

With 30-50 variants:

- **Spearman correlation:** ~0.4-0.6 (moderate)    task_type='code',

- **RMSE:** Varies by dataset

- **Use case:** Basic pattern detection, variant ranking    api_key=os.getenv('GEMINI_API_KEY'),**QA Pipeline**:



For paper-level performance (0.89), you need:    threshold=0.6,- `RequirementAnalyzer`: Analyzes question

- 1,000+ variants

- Multiple MAS architectures    max_retries=2- `QAAnswerer`: Generates answer

- Multiple LLM models

- Full benchmark samples)- `QAReviewer`: Reviews answer quality



---



## ğŸ¯ Quick Commands# features contains 16 metrics### 2. Agent Monitor (`Agent_Monitor/agent_monitor.py`)



```bashprint(features)

# Run demo

python examples/complete_demo.py# {**6-Dimensional Scoring**:



# Generate variants#   'avg_personal_score': 0.75,1. **personal_score**: Overall quality (0-1)

python scripts/generate_mas_variants.py

#   'min_personal_score': 0.60,2. **factual_accuracy**: Correctness (0-1)

# Run simple integration

python examples/simple_integration.py#   'max_loops': 2,3. **clarity**: Readability (0-1)



# Install dependencies#   ...4. **safety**: Security/ethics (0-1)

pip install -r requirements.txt

#   'collective_score': 0.825. **code_correctness**: Syntax/logic (0-1, code only)

# Create virtual environment

python -m venv venv# }6. **complexity**: Appropriate complexity (0-1)

venv\Scripts\activate  # Windows

``````



---**Enhancement Loop**:



## ğŸ“Š Example Output### Step 2: Feature Aggregation```python



### Feature Extractionwhile score < threshold and loops < max_retries:

```json

{For each MAS variant:    enhanced_output = llm.enhance(current_output, suggestions)

  "avg_personal_score": 0.82,

  "min_personal_score": 0.75,1. Run on 10 HumanEval tasks â†’ collect 10 feature sets    score = llm.score(enhanced_output)

  "max_loops": 2,

  "total_latency": 4.5,2. Run on 10 GSM8K tasks â†’ collect 10 feature sets    loops += 1

  "total_token_usage": 1500,

  "num_agents_triggered_enhancement": 1,3. Run on 10 MMLU tasks â†’ collect 10 feature sets```

  "num_nodes": 3,

  "num_edges": 2,4. **Aggregate**: Average each of 15 features across all 30 runs

  "avg_clustering": 0.0,

  "global_transitivity": 0.0,5. Compute benchmark scores (accuracy on each benchmark)### 3. Feature Aggregator (`Agent_Monitor/feature_aggregator.py`)

  "avg_degree_centrality": 0.67,

  "avg_betweenness_centrality": 0.33,6. Compute label: `0.5 Ã— humaneval + 0.3 Ã— gsm8k + 0.2 Ã— mmlu`

  "avg_closeness_centrality": 0.83,

  "pagerank_entropy": 1.09,**19 Features Collected**:

  "heterogeneity_score": 0.15,

  "collective_score": 0.78### Step 3: XGBoost Training

}

```**System Metrics** (8):



### Benchmark Evaluation```python- `avg_personal_score`, `max_personal_score`, `min_personal_score`

```json

{# X: 16 features (15 + collective_score)- `total_latency_sec`, `avg_token_count`

  "humaneval_score": 0.65,

  "gsm8k_score": 0.72,# y: label_mas_score- `max_loops`, `avg_loops`

  "mmlu_score": 0.58,

  "label": 0.663- `collective_score` (LLM-based system-level score)

}

```model = xgb.XGBRegressor(



### Prediction    n_estimators=100,**Graph Metrics** (11):

```

Predicted MAS score: 0.687    max_depth=5,- `num_nodes`, `num_edges`, `avg_degree`

Feature importance:

  1. collective_score: 0.25    learning_rate=0.1- `clustering_coefficient`, `transitivity`

  2. avg_personal_score: 0.18

  3. num_nodes: 0.12)- `avg_betweenness_centrality`, `avg_closeness_centrality`

  4. total_latency: 0.10

  ...model.fit(X, y)- `pagerank_entropy`, `authority_entropy`

```

```- `density`, `diameter`

---



## ğŸ“– Citation

### Step 4: Prediction**Benchmark Scores** (3):

If you use this implementation, please cite the original paper:

- `humaneval_score` (code correctness)

```bibtex

@article{chan2024agentmonitor,```python- `gsm8k_score` (math reasoning)

  title={AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems},

  author={Chan, Chi-Min and Huang, Jianxuan and Liu, Weize and Lyu, Xinle and Liu, Zikang and Yang, Shuyu and Liu, Jiaxuan and Zhou, Yixin and Chen, Qianyu and Wang, Chunyang and others},# New MAS execution- `mmlu_score` (knowledge)

  journal={arXiv preprint arXiv:2408.14972},

  year={2024}new_features = [0.68, 0.45, 2, 27.5, 1500, 3, 5, 4, 0.20, ...]  # 16 values

}

```### 4. XGBoost Model (`Trainer/xgb_trainer.py`)



**Paper:** https://arxiv.org/abs/2408.14972  # Predict performance

**Official Repo:** https://github.com/chanchimin/AgentMonitor

predicted_score = model.predict([new_features])**Model Configuration**:

---

print(f"Predicted MAS Score: {predicted_score[0]:.4f}")```python

## ğŸ¤ Contributing

```XGBRegressor(

Issues and pull requests welcome! This is a research implementation - there's room for improvement:

    n_estimators=100,

- [ ] Add more MAS frameworks (AutoGen, LangChain, ChatDev)

- [ ] Implement safety post-editing module---    max_depth=5,

- [ ] Add more benchmark datasets

- [ ] Optimize feature extraction (caching)    learning_rate=0.1,

- [ ] Create web dashboard (Streamlit/Gradio)

## ğŸ“ˆ Expected Performance    objective='reg:squarederror'

---

)

## ğŸ“„ License

### Training Data```

MIT License - See LICENSE file for details.

- **Rows**: 5 MAS variants

---

- **RÂ² Score**: 0.7-0.9 (on simulated data)**Training Requirements**:

## ğŸ™ Acknowledgments

- **MAE**: 0.05-0.15- Minimum 10 rows (50-100 recommended)

- Research paper authors for the methodology

- Official AgentMonitor repository for inspiration- 18 input features â†’ 1 target (`collective_score`)

- NetworkX team for graph analysis tools

- XGBoost team for the prediction framework### Use Cases- Cross-validation: 5-fold KFold



---- **Quick MAS evaluation**: Predict performance without running expensive benchmarks- Saved as `models/xgb_model.json`



**Built with â¤ï¸ for Multi-Agent System research and production**- **MAS configuration tuning**: Test different thresholds/retries



*Last Updated: October 11, 2025*- **Agent monitoring**: Track system health during execution---




---## ğŸ“– Usage Guide



## ğŸ› ï¸ Troubleshooting### Run Single Prompt



### Issue: Import Errors```powershell

```bashpython Agent_Monitor/run_with_monitor.py

# Solution```

pip install -r requirements.txt

```**Example**:

```

### Issue: API Key ErrorsEnter task: write python code for fibonacci sequence

```bash

# Solution: Set environment variableOutput:

$env:GEMINI_API_KEY = "your-key"âœ… RequirementAnalyzer: Score 0.85

âœ… CodeGenerator: Score 0.92  

# Or modify evaluate_mas_on_benchmarks.py to use Ollamaâœ… CodeReviewer: Score 0.88

```âœ… UnitTestWriter: Score 0.79

âœ… CodeExecutor: Syntax OK

### Issue: Benchmark Data Not Found

```bashFeatures saved to: data/features.csv

# Ensure these folders exist:Logs: logs/run_20251008_153042.json

BenchmarkDatasetFolder/HumanEval/data.csv```

BenchmarkDatasetFolder/GSM8k/data.csv

BenchmarkDatasetFolder/MMLU/data.csv### Generate Training Data

```

```powershell

### Issue: Execution Too Slowpython Agent_Monitor/benchmark_runner.py

```bash```

# Use fewer samples (5 instead of 10)

# In evaluate_mas_on_benchmarks.py, when prompted:**Options**:

Samples per benchmark: 5- Select dataset: HumanEval, GSM8K, MMLU

```- Number of samples: 10, 50, 100

- Output: Appends to `data/features.csv`

---

### Train XGBoost Model

## ğŸ“š Key Files Explained

```powershell

### `agent_monitor.py`python Trainer/xgb_trainer.py

Monitors MAS execution, tracks agent interactions, implements enhancement loops.```



### `run_with_monitor.py`**Requirements**:

Main entry point. Executes MAS pipeline with monitoring enabled, returns features.- At least 10 rows in `data/features.csv`

- Varying `collective_score` values

### `feature_aggregator.py`

Extracts 16 features from MAS execution logs. Computes graph metrics, system metrics, and collective score.**Output**:

```

### `evaluate_mas_on_benchmarks.py`âœ… Model trained with 5-fold CV

Generates training data by running MAS variants on benchmarks. Creates 20-column CSV.âœ… RÂ² Score: 0.73

âœ… RMSE: 0.15

### `xgb_trainer_mas.py`âœ… Saved to: models/xgb_model.json

Trains XGBoost model on 20-column CSV using first 16 columns as features.```



### `predict_mas.py`### Make Predictions

Loads trained model and makes predictions on new MAS feature vectors.

```powershell

---python Trainer/predict.py

```

## ğŸ“ Research Background

**Input**: 18 features (from latest run)  

This system implements a **weak supervision** approach for MAS evaluation:**Output**: Predicted `collective_score` (0-1)



1. **Problem**: Evaluating MAS on benchmarks is expensive (time, tokens, API costs)---

2. **Solution**: Train a predictor using cheap features (execution metrics)

3. **Training**: Use benchmark scores as labels (weak supervision)## ğŸ”§ Fixes Applied

4. **Inference**: Predict performance from features alone (no benchmarks needed)

### Issue #1: Gemini API 404 Error âœ… FIXED

### Weak Supervision Formula

```**Problem**: Using deprecated model `gemini-1.5-flash`

label_mas_score = 0.5 Ã— humaneval_score + 

                  0.3 Ã— gsm8k_score + **Solution**: Updated to `gemini-2.0-flash`

                  0.2 Ã— mmlu_score

```**Files Modified**:

- `Agent_Monitor/agent_monitor.py` (line 15)

Weights reflect task importance:- `Agent_Monitor/run_with_monitor.py` (line 29)

- **50%** Code generation (most important for coding MAS)

- **30%** Math reasoning (logical thinking)### Issue #2: All Scores Returning 0.0 âœ… FIXED

- **20%** General knowledge (factual accuracy)

**Problem**: Gemini wraps JSON in markdown code blocks (` ```json {...} ``` `), causing `json.loads()` to fail

---

**Solution**: Added `extract_json()` function to strip markdown

## ğŸ¤ Contributing

**Code Added** (`agent_monitor.py`):

To extend this system:```python

def extract_json(raw_text: str) -> str:

1. **Add more features**: Modify `feature_aggregator.py`    raw_text = raw_text.strip()

2. **Add benchmarks**: Update `evaluate_mas_on_benchmarks.py`    if raw_text.startswith("```json"):

3. **Try different models**: Replace XGBoost in `xgb_trainer_mas.py`        raw_text = raw_text[7:]

4. **Tune hyperparameters**: Adjust XGBoost settings    elif raw_text.startswith("```"):

        raw_text = raw_text[3:]

---    if raw_text.endswith("```"):

        raw_text = raw_text[:-3]

## ğŸ“ License    return raw_text.strip()

```

This project is part of research on Multi-Agent System evaluation and monitoring.

### Issue #3: collective_score Always 0.5 âœ… FIXED

---

**Problem**: LLM client not passed to `get_collective_score_with_llm()`

## ğŸ“§ Support

**Solution**: 

For questions or issues:1. Added `extract_json()` to feature_aggregator.py

1. Check `INSTRUCTIONS_FOR_FRIEND.md` for dataset generation guide2. Added debug logging to track LLM calls

2. Review logs in `logs/` directory3. Ensured `llm_client` passed from `run_with_monitor.py`

3. Verify CSV format matches `data/mas_benchmark_results_FORMAT_GUIDE.csv`

### Issue #4: max_loops Always 2 âœ… FIXED

---

**Problem**: Threshold (0.8) higher than typical scores (0.0-0.6)

## âœ… Quick Start Checklist

**Solution**: Lowered threshold from 0.8 â†’ 0.6

- [ ] Install Python 3.8+

- [ ] Create virtual environment**Impact**: Fewer wasted enhancement loops, `max_loops` now varies

- [ ] Install dependencies: `pip install -r requirements.txt`

- [ ] Set API key: `$env:GEMINI_API_KEY = "your-key"`### Issue #5: Generated Code Has Markdown âœ… FIXED

- [ ] Generate data: `python Agent_Monitor/evaluate_mas_on_benchmarks.py`

- [ ] Train model: `python Trainer/xgb_trainer_mas.py`**Problem**: Code wrapped in ` ```python ... ``` ` causing syntax errors

- [ ] Make predictions: `python Trainer/predict_mas.py`

**Solution**: Strip markdown blocks from generated code

---

**Code Added** (`mas_pipeline.py`):

**Built with â¤ï¸ for efficient Multi-Agent System evaluation**```python

if code.startswith("```python"):
    code = code[9:]
elif code.startswith("```"):
    code = code[3:]
if code.endswith("```"):
    code = code[:-3]
```

---

## ğŸ“Š Data Analysis

### Training Data Overview

**Current Status** (as of Oct 8, 2025):
- **Total rows**: 23
- **Features**: 19 columns
- **Source**: HumanEval benchmark runs

### Column Analysis

**Constant Columns** (Expected - Same pipeline structure):
- `num_nodes`: 5 (code) or 3 (QA)
- `num_edges`: 4 or 2
- `clustering_coefficient`: 0.0
- `transitivity`: 0
- `avg_betweenness_centrality`: 0.167

**Varying Columns** (Good):
- `avg_personal_score`: 0.0 - 0.59
- `total_latency_sec`: 29.8 - 92.1
- `avg_token_count`: 15 - 89
- `max_loops`: Now varies (0, 1, 2) after fix
- `collective_score`: Now varies (0.3 - 0.8) after fix

**Zero Columns** (No tasks run yet):
- `gsm8k_score`: 0.0 (100%)
- `mmlu_score`: 0.0 (100%)

### Recommendations

1. **Generate more data**: Need 50-100 rows for robust XGBoost model
2. **Diversify tasks**: Run GSM8K and MMLU benchmarks
3. **Vary pipeline structure**: Try parallel agents, conditional flows for graph metric diversity

---

## ğŸ› Troubleshooting

### Gemini API Errors

**404 Error - Model not found**:
```
Error: models/gemini-1.5-flash is not found
Solution: Update to gemini-2.0-flash
```

**JSON Parsing Errors**:
```
Error: json.loads() failed
Solution: Use extract_json() to strip markdown
```

### Ollama Issues

**Connection refused**:
```powershell
# Check if Ollama is running
ollama list

# Start Ollama service
ollama serve
```

**Model not found**:
```powershell
# Pull model
ollama pull llama3
```

### XGBoost Training Fails

**Not enough data**:
```
Error: Need at least 10 rows
Solution: Run benchmark_runner.py to generate more data
```

**constant target variable**:
```
Error: collective_score is constant
Solution: Ensure LLM client is passed correctly (check fix #3)
```

### Enhancement Loop Issues

**All agents hit max_retries**:
```
Issue: max_loops always 2
Solution: Lower threshold (0.8 â†’ 0.6) or increase max_retries
```

---

## ğŸ“ Project Structure

```
Final/
â”œâ”€â”€ Agent_Monitor/
â”‚   â”œâ”€â”€ agent_monitor.py          # Core monitoring + scoring
â”‚   â”œâ”€â”€ feature_aggregator.py     # 19-feature extraction
â”‚   â”œâ”€â”€ run_with_monitor.py       # Main orchestrator
â”‚   â”œâ”€â”€ benchmark_runner.py       # Dataset evaluation
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ eval_utils.py         # Benchmark scoring
â”‚       â”œâ”€â”€ graph_utils.py        # NetworkX analytics
â”‚       â””â”€â”€ json_logger.py        # JSON logging
â”œâ”€â”€ MAS/
â”‚   â””â”€â”€ mas_pipeline.py           # Multi-agent pipelines
â”œâ”€â”€ Trainer/
â”‚   â”œâ”€â”€ xgb_trainer.py            # XGBoost training
â”‚   â””â”€â”€ predict.py                # Model inference
â”œâ”€â”€ BenchmarkDatasetFolder/
â”‚   â”œâ”€â”€ HumanEval/data.csv        # 5,893 code tasks
â”‚   â”œâ”€â”€ GSM8K/data.csv            # 6,142 math problems
â”‚   â””â”€â”€ MMLU/data.csv             # Knowledge questions
â”œâ”€â”€ data/
â”‚   â””â”€â”€ features.csv              # Training data (23 rows)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ xgb_model.json            # Trained XGBoost
â”œâ”€â”€ logs/                         # JSON run logs
â”œâ”€â”€ .env                          # API keys + config
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

---

## ğŸ”‘ Key Commands

```powershell
# Environment setup
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt

# Run tasks
python Agent_Monitor/run_with_monitor.py          # Single prompt
python Agent_Monitor/benchmark_runner.py          # Generate data
python Trainer/xgb_trainer.py                     # Train model
python Trainer/predict.py                         # Predict score

# Switch LLM provider
# In .env: LLM_PROVIDER=gemini or ollama

# Ollama setup
pip install ollama
ollama pull llama3
ollama run llama3 "test"
```

---

## ğŸ“ˆ Workflow: Dataset Generation â†’ Model Training

### **Phase 1: Dataset Generation (Friend with Llama/Ollama)**

**Your friend will:**
1. Setup Ollama with local Llama model (see `DATASET_GENERATION_GUIDE.md`)
2. Run benchmark evaluation:
   ```powershell
   python Trainer/evaluate_mas_on_benchmarks.py
   # Choose 20-50 samples per benchmark
   ```
3. Share generated file: `data/mas_benchmark_results.csv`

**Time**: 2-8 hours (depending on sample size)  
**Cost**: FREE (local model)

---

### **Phase 2: Model Training (You)**

**After receiving dataset from friend:**

1. **Copy dataset to your system:**
   ```powershell
   # Place mas_benchmark_results.csv in data/ folder
   ```

2. **Train XGBoost model:**
   ```powershell
   .\venv\Scripts\Activate.ps1
   python Trainer/xgb_trainer_mas.py
   ```

3. **Test predictions:**
   ```powershell
   python Trainer/predict_mas.py
   ```

4. **Deploy model:**
   - Predict MAS_score for new configurations
   - Optimize MAS design based on predictions
   - A/B test different agent combinations

**Time**: 10 minutes  
**Cost**: FREE

---

### **Why This Workflow?**

âœ… **Friend has GPU/free time** â†’ Generates expensive benchmark data  
âœ… **You get trained model** â†’ No API costs for you  
âœ… **Everyone benefits** â†’ Collaborative approach to AI research  

See `DATASET_GENERATION_GUIDE.md` for complete instructions!

---

## ğŸ“ License

MIT License - See repository for details

## ğŸ¤ Contributing

Contributions welcome! Please submit PRs to the `shruthi` branch.

## ğŸ“§ Contact

**Author**: Kumaraswamy Bakkashetti  
**Repository**: https://github.com/KumaraswamyBakkashetti/3-1project  
**Branch**: shruthi

---

**Last Updated**: October 8, 2025  
**Version**: 2.0 (Ollama-ready)
